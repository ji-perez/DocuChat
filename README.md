# **Proyecto: Sistema de Preguntas y Respuestas sobre Documentos Propios 游눫**

Versi칩n: 1.0  
Fecha: 2025-08-07  
Nombre: DocuChat

## **1\. Descripci칩n del Proyecto**

### **쮻e qu칠 trata?**

Este proyecto consiste en el desarrollo de una aplicaci칩n web que permite a los usuarios cargar sus propios documentos (como archivos PDF o de texto) y mantener una conversaci칩n con ellos. En esencia, se trata de un "chatbot" especializado que utiliza el contenido de un documento espec칤fico como su 칰nica fuente de conocimiento para responder preguntas.

### **Objetivo Principal**

El objetivo es construir un sistema de **Retrieval-Augmented Generation (RAG)** funcional y desacoplado. Esto permitir치 a los usuarios extraer informaci칩n y obtener respuestas precisas a preguntas complejas sobre sus documentos sin necesidad de leerlos por completo, aprovechando el poder de los Grandes Modelos de Lenguaje (LLMs).

### **Funcionalidades Clave**

* **Carga de Documentos**: La interfaz de usuario permitir치 al usuario seleccionar y subir un documento (inicialmente, .pdf y .txt).  
* **Procesamiento de Documentos**: El sistema procesar치 el documento cargado, lo dividir치 en fragmentos manejables y crear치 representaciones vectoriales (embeddings) de cada fragmento.  
* **Interfaz de Preguntas y Respuestas**: Un 치rea de chat donde el usuario puede escribir una pregunta.  
* **Generaci칩n de Respuestas**: El sistema buscar치 los fragmentos m치s relevantes del documento para responder a la pregunta del usuario y utilizar치 un LLM para generar una respuesta coherente y contextualizada.

---

## **2\. Tecnolog칤as Utilizadas**

El proyecto se dividir치 en dos componentes principales: un backend para la l칩gica de IA y un frontend para la interacci칩n con el usuario.

### **Backend (API de L칩gica)**

* **Python 3.9+**: El lenguaje de programaci칩n base para todo el desarrollo.  
* **LangChain**: El framework principal para orquestar todos los componentes de la l칩gica de IA. Se usar치 para cargar documentos (DocumentLoaders), dividirlos (TextSplitters), interactuar con el modelo de embeddings y el LLM, y construir la cadena de recuperaci칩n (RetrievalQA).  
* **FastAPI**: Un framework web de alto rendimiento para construir el API REST. Ser치 el encargado de exponer la l칩gica de LangChain a trav칠s de endpoints HTTP, gestionando las peticiones del frontend.  
* **OpenAI**: Se utilizar치 su API para acceder a dos servicios clave:  
  * **Modelos de Embeddings** (text-embedding-3-small): Para convertir los fragmentos de texto en vectores num칠ricos.  
  * **Modelo de Lenguaje (LLM)** (gpt-3.5-turbo o gpt-4o): El cerebro que generar치 las respuestas en lenguaje natural.  
* **ChromaDB**: Una base de datos vectorial de c칩digo abierto. Almacenar치 los vectores de los fragmentos de texto y permitir치 realizar b칰squedas de similitud sem치ntica de manera eficiente.  
* **Uvicorn**: Un servidor ASGI (Asynchronous Server Gateway Interface) que se usar치 para ejecutar la aplicaci칩n FastAPI.
* **Anaconda**: La generaci칩n y gesti칩n de los ambientes virtuales se hara con Anaconda.

### **Frontend (Interfaz de Usuario)**

* **Streamlit**: Un framework de Python que permite crear aplicaciones web interactivas con muy poco c칩digo. Ideal para construir la interfaz de usuario de forma r치pida y sencilla.  
* **Requests**: Una librer칤a de Python est치ndar para realizar peticiones HTTP. Streamlit la usar치 para comunicarse con el backend de FastAPI.

---

## **3\. Arquitectura del Sistema**

Adoptaremos una **arquitectura cliente-servidor** desacoplada, lo que nos brinda escalabilidad y modularidad.

[Imagen de a client-server architecture diagram](https://encrypted-tbn0.gstatic.com/licensed-image?q=tbn:ANd9GcRfljlOrd8cDb5ShcecLetBMKDNiDVc21MOiBfDtfGEI1sowEuXk69puLw05wrWSphYjcR1G7qwP50eQq9933sh53SmatFWyyqRvuhQjJWFG45Ee4A)

* **Backend (Servidor)**: Es una aplicaci칩n **FastAPI** que act칰a como el motor de la inteligencia artificial. Su 칰nica responsabilidad es recibir una pregunta, procesarla a trav칠s del flujo de LangChain y devolver una respuesta. No sabe nada sobre c칩mo se ve la interfaz de usuario. Contiene el LLM, la base de datos de vectores y toda la l칩gica de RAG.  
* **Frontend (Cliente)**: Es una aplicaci칩n **Streamlit** que se ejecuta en el navegador del usuario (o en un servidor web). Su responsabilidad es renderizar la interfaz gr치fica, capturar la entrada del usuario (la pregunta) y mostrar la respuesta. No realiza ning칰n procesamiento de IA; simplemente se comunica con el backend a trav칠s de su API.

### **Flujo de Comunicaci칩n:**

1. El usuario escribe una pregunta en la interfaz de Streamlit y pulsa "Enviar".  
2. El script de Streamlit empaqueta la pregunta en una carga 칰til JSON.  
3. Streamlit env칤a una petici칩n POST al endpoint /ask del backend de FastAPI.  
4. FastAPI recibe la petici칩n, extrae la pregunta y la pasa a la cadena RetrievalQA de LangChain.  
5. LangChain:  
   a. Convierte la pregunta del usuario en un vector.  
   b. Busca en ChromaDB los fragmentos de texto cuyos vectores son m치s similares al vector de la pregunta.  
   c. Env칤a los fragmentos recuperados y la pregunta original al LLM de OpenAI.  
6. El LLM genera una respuesta basada en el contexto proporcionado.  
7. FastAPI recibe la respuesta del LLM y la devuelve al frontend en formato JSON.  
8. Streamlit recibe la respuesta, la procesa y la muestra en la interfaz para que el usuario la lea.

---

## **4\. Plan de Desarrollo (Paso a Paso)**

### **Paso 1: Configuraci칩n del Entorno**

1. **Crear un directorio para el proyecto**: langchain\_qa\_project/.  
2. Dentro, crear dos subdirectorios: backend/ y frontend/.  
3. **Crear un entorno virtual (usando Anaconda) de Python para el backend y otro para el frontend**:  
4. **Instalar todas las dependencias**:  
   Bash  
   pip install "fastapi\[all\]" uvicorn langchain langchain-openai langchain-chroma pypdf streamlit requests python-dotenv

5. **Obtener API Key de OpenAI**: Crear una cuenta en la plataforma de OpenAI, generar una clave de API y guardarla en un archivo .env en la ra칤z del proyecto para mantenerla segura.

### **Paso 2: Desarrollo del Backend (en backend/)**

1. **Crear el archivo principal**: main.py.  
2. **Implementar la l칩gica de LangChain**: Crear una funci칩n o clase que se encargue de:  
   * Cargar un documento desde una ruta de archivo.  
   * Dividirlo en trozos (chunks) usando RecursiveCharacterTextSplitter.  
   * Crear los embeddings para cada trozo usando OpenAIEmbeddings.  
   * Almacenar los trozos y sus embeddings en **ChromaDB**.  
   * Inicializar una cadena RetrievalQA.  
3. **Crear el API con FastAPI**:  
   * Importar FastAPI y pydantic para definir los modelos de petici칩n y respuesta.  
   * Crear un endpoint POST /process-document que reciba la ruta de un archivo, lo procese y lo guarde en la base de datos vectorial.  
   * Crear un endpoint POST /ask que reciba una pregunta (question) y use la cadena RetrievalQA para generar y devolver una respuesta.  
4. **Probar el backend de forma aislada**: Usar herramientas como la documentaci칩n autom치tica de FastAPI (/docs) o curl para enviar peticiones y verificar que funciona correctamente.

### **Paso 3: Desarrollo del Frontend (en frontend/)**

1. **Crear el archivo principal**: app.py.  
2. **Dise침ar la interfaz con Streamlit**:  
   * A침adir un t칤tulo y una descripci칩n.  
   * Usar st.file\_uploader para permitir al usuario subir un archivo PDF o TXT.  
   * Una vez subido el archivo, llamar al endpoint /process-document del backend para que lo procese. Mostrar un mensaje de 칠xito.  
   * A침adir un campo de texto (st.text\_input) para la pregunta del usuario y un bot칩n (st.button).  
3. **Conectar con el Backend**:  
   * Cuando se presione el bot칩n, usar la librer칤a requests para enviar la pregunta al endpoint /ask del backend.  
   * Mostrar un spinner (st.spinner) mientras se espera la respuesta.  
   * Mostrar la respuesta recibida en la interfaz usando st.write o st.success.  
   * A침adir manejo de errores por si el backend no est치 disponible.

### **Paso 4: Integraci칩n y Pruebas**

1. **Ejecutar ambos servicios simult치neamente**:  
   * En un terminal, iniciar el backend: cd backend && uvicorn main:app \--reload  
   * En otro terminal, iniciar el frontend: cd frontend && streamlit run app.py  
2. **Realizar pruebas de extremo a extremo**:  
   * Abrir la app de Streamlit en el navegador.  
   * Subir un documento.  
   * Hacer varias preguntas (simples, complejas, sobre detalles espec칤ficos).  
   * Verificar que las respuestas son coherentes y se basan en el contenido del documento.

### **Paso 5 (Opcional): Contenerizaci칩n con Docker**

1. **Crear un Dockerfile para el backend**.  
2. **Crear un Dockerfile para el frontend**.  
3. **Crear un archivo docker-compose.yml** para orquestar y ejecutar ambos contenedores con un solo comando (docker-compose up), facilitando el despliegue y la portabilidad del proyecto.